{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1c9eb2",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6119a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def scaled_dot_product(Q, K, V, mask=None):\n",
    "    dk = K.shape[-1] * 1.0 ## To convert the dimension integer number to flaot\n",
    "    #print(f\"K Shape: {K.shape}\")\n",
    "    attention_scores = tf.matmul(Q, tf.transpose(K, perm=[0, 1, 3, 2])) / tf.sqrt(dk)\n",
    "    #print(f\"attention_scores shape: {attention_scores.shape}, mask shape: {mask.shape}\")\n",
    "    if mask is not None:\n",
    "        attention_scores += mask\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "    attention = attention_weights @ V\n",
    "    \n",
    "    return attention_weights, attention\n",
    "\n",
    "class PositionalEncoding(tf.Module):\n",
    "    def __init__(self, max_seq_len, token_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_dim = token_dim\n",
    "        \n",
    "    def __call__(self):\n",
    "\n",
    "        even_denom = tf.math.pow(tf.cast(10000,tf.float64),  tf.range(0, self.token_dim, 2)/ self.token_dim)\n",
    "        odd_denom = tf.math.pow(tf.cast(10000,tf.float64),  tf.range(1, self.token_dim, 2)/ self.token_dim)\n",
    "        position = tf.expand_dims(tf.range(0, self.max_seq_len, 1, dtype=tf.float64),-1)\n",
    "        pe_even = tf.sin(position / even_denom)\n",
    "        pe_odd = tf.cos(position / odd_denom)\n",
    "        pe = tf.stack([pe_even, pe_odd], axis=2)\n",
    "        pe = tf.reshape(pe, [self.max_seq_len, self.token_dim])\n",
    "        pe = tf.cast(pe, tf.float32)\n",
    "        \n",
    "        return pe\n",
    "\n",
    "class MultiheadAttention(tf.Module):\n",
    "    \n",
    "    def __init__(self, nheads, sequence_len, seq_dim):\n",
    "        super().__init__(name=None)\n",
    "        \n",
    "        self.nheads = nheads\n",
    "        self.sequence_len = sequence_len\n",
    "        self.seq_dim = seq_dim\n",
    "        self.head_dim = self.seq_dim // self.nheads\n",
    "        self.qkv_net = tf.keras.layers.Dense(3 * self.seq_dim, name='qkv_net')\n",
    "        self.fcnn = tf.keras.layers.Dense(self.seq_dim, name='mhfcnn')\n",
    "        self.built = False\n",
    "        self.attention_weights = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __call__(self, X, mask=None):\n",
    "        \n",
    "        batch_size = tf.shape(X)[0]\n",
    "        \n",
    "        ## Create a network to generate QKV matrices\n",
    "        assert self.seq_dim == X.shape[-1], \"seaquence dimension given and sequence dimension in the input data is not matching \"\n",
    "        #print(f\"X shape: {X.shape}\")\n",
    "        QKV = self.qkv_net(X)\n",
    "        #print(f\"QKV shape from qkv net: {QKV.shape}\")\n",
    "        QKV = tf.reshape(QKV, [batch_size, self.sequence_len]+[self.nheads, QKV.shape[-1] // self.nheads])\n",
    "        QKV = tf.transpose(QKV, perm=[0,2,1,3])\n",
    "        #print(f\"QKV shape after heads added: {QKV.shape}\")\n",
    "        Q, K, V = tf.split(QKV, 3, axis=-1)\n",
    "        #print(f\"QKV shape individually: {(Q.shape, K.shape, V.shape)}\")\n",
    "        self.attention_weights, attention_embeddings = scaled_dot_product(Q, K, V, mask)\n",
    "        #print(f\"Attention weights and embeddings shape: {(self.attention_weights.shape, attention_embeddings.shape)}\")\n",
    "        #batch_size = attention_embeddings.shape[0]\n",
    "        attention_embeddings = tf.reshape(attention_embeddings, shape=[batch_size,self.sequence_len,self.nheads*self.head_dim])\n",
    "        #print(f\"Attention embeddings shape before NN: {(self.attention_weights.shape, attention_embeddings.shape)}\")\n",
    "        attention_out  = self.fcnn(attention_embeddings)\n",
    "        #print(f\"final attention block output shape: {attention_out.shape}\")\n",
    "        \n",
    "        return attention_out\n",
    "        \n",
    "        ## Break that into \n",
    "        \n",
    "        \n",
    "class PositionwiseFeedForward(tf.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob):\n",
    "        super().__init__(name=None)\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden, name='pffl1')\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model, name='pffl2')\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(f\"x after first linear layer: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        #print(f\"x after activation: {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.shape}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.shape}\")\n",
    "        return x    \n",
    "    \n",
    "class EncoderBlock(tf.Module):\n",
    "    def __init__(self, nheads, sequence_len, seq_dim, hidden_units, drop_prob):\n",
    "        super().__init__(name=None)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(name='lnorm1')\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(name='lnorm2')\n",
    "        self.mha = MultiheadAttention( nheads, sequence_len, seq_dim)\n",
    "        self.pff = PositionwiseFeedForward(seq_dim, hidden_units, drop_prob)\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, mask=None):\n",
    "        X_A = self.layer_norm1(X)\n",
    "        X_A = self.mha(X_A, mask)\n",
    "        X_A = X + X_A\n",
    "        X_F = self.layer_norm2(X_A)\n",
    "        X_F = self.pff(X_F)\n",
    "        X_out = X_A + X_F\n",
    "        \n",
    "        ## collectign trainable variables\n",
    "        \"\"\"\n",
    "        self.trainable_variables = self.layer_norm1.trainable_variables + \\\n",
    "                                        self.mha.trainable_variables + \\\n",
    "                                            self.layer_norm2.trainable_variables + \\\n",
    "                                                self.pff.trainable_variables\n",
    "        \"\"\"\n",
    "        return X_out\n",
    "        \n",
    "class Encoder(tf.Module):\n",
    "    def __init__(self, nblocks, nheads, sequence_len, seq_dim, hidden_units, drop_prob=0.1 ):\n",
    "        self.encoder_blocks = [EncoderBlock(nheads, sequence_len, seq_dim, hidden_units, drop_prob) for _ in range(nblocks)]\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, mask=None):\n",
    "        ##\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            X = encoder_block(X, mask)\n",
    "        return X\n",
    "    \n",
    "class Preprocessing():\n",
    "    def __init__(self, max_vocab, max_sequence_len):\n",
    "        self.vectorizer = tf.keras.layers.TextVectorization(            \n",
    "            max_tokens = max_vocab,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=max_sequence_len\n",
    "        )\n",
    "        \n",
    "    def adapt(self, X):\n",
    "        self.vectorizer.adapt(X)\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.vectorizer(X)\n",
    "        \n",
    "        \n",
    "class CBERT(tf.keras.Model):\n",
    "    def __init__(self, heads, endocer_layers, preprocessing_model, max_vocab, max_sequence_len, embedding_dim, hidden_units, dropout_rate):\n",
    "        super().__init__(name=None)\n",
    "        self.max_vocab = max_vocab\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.heads = heads        \n",
    "        self.vectorizer = preprocessing_model\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(max_vocab, embedding_dim, mask_zero=True)\n",
    "        self.pe = PositionalEncoding(max_sequence_len, embedding_dim)\n",
    "        self.encoder = Encoder(\n",
    "            nblocks=endocer_layers, nheads=heads, sequence_len= max_sequence_len,\n",
    "            seq_dim=embedding_dim, hidden_units=hidden_units, drop_prob=dropout_rate\n",
    "                         )\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_metric = tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "        self.test_accuracy_metric = tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "       \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        \n",
    "        batch_size = tf.shape(X)[0]\n",
    "        embeddings = self.embedding_layer(X)\n",
    "        input_mask = self.embedding_layer.compute_mask(X)\n",
    "\n",
    "        #get positional encodings \n",
    "        positional_encodings = self.pe()\n",
    "\n",
    "        #Now make the final input to the Encode part of the transformer\n",
    "        # That is embeddings + positional embeddings\n",
    "\n",
    "        X = embeddings + positional_encodings\n",
    "        ## reshaping input mask required for the encoder\n",
    "        ## in the shape [batch, heads, sequnce_len, sequnce_len]\n",
    "        ## middle two dimenstio are keepign ones as matrix operation automatically broadcast\n",
    "        input_mask_for_encoder = tf.where(input_mask==True, 0.0, float(\"-inf\"))        \n",
    "        input_mask_for_encoder = tf.reshape(input_mask_for_encoder, shape=[batch_size,1,1,input_mask_for_encoder.shape[-1]])\n",
    "        encoder_output = self.encoder(X, input_mask_for_encoder)\n",
    "\n",
    "        ##averaging the embeddings of all tokens in the sequence\n",
    "        averaged_embeddings = tf.keras.layers.GlobalAveragePooling1D()(encoder_output)\n",
    "        ## output probabilities \n",
    "        y_hat = self.output_layer(averaged_embeddings)\n",
    "        \n",
    "        return y_hat\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train_step(self, data):        \n",
    "        \n",
    "        X,y = data\n",
    "        #y = tf.expand_dims(y, axis=-1)\n",
    "          \n",
    "        #Vectorize the raw texts    \n",
    "        X = self.vectorizer(X)\n",
    "\n",
    "        #convert the word vectors into embeddings\n",
    "        with tf.GradientTape() as tape:            \n",
    "            y_pred = self(X)            \n",
    "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "        \n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        \n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.accuracy_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy_metric.result()}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data        \n",
    "        #Vectorize the raw texts    \n",
    "        x = self.vectorizer(x)\n",
    "        # Compute predictions\n",
    "        y_pred = self(x)\n",
    "        # Updates the metrics tracking the loss\n",
    "        loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "        # Update the metrics.\n",
    "        for metric in self.metrics:\n",
    "            if metric.name != \"loss\":\n",
    "                metric.update_state(y, y_pred)\n",
    "        self.test_accuracy_metric.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {\"loss\":loss, \"accuracy\":self.test_accuracy_metric.result()}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abd012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
