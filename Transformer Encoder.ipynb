{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b17f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c9eb2",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f6119a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scaled_dot_product(Q, K, V, mask=None):\n",
    "    dk = K.shape[-1] * 1.0 ## To convert the dimension integer number to flaot\n",
    "    print(f\"K Shape: {K.shape}\")\n",
    "    attention_scores = tf.matmul(Q, tf.transpose(K, perm=[0, 1, 3, 2])) / tf.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        attention_scores += mask\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "    attention = attention_weights @ V\n",
    "    \n",
    "    return attention_weights, attention\n",
    "\n",
    "class PositionalEncoding(tf.Module):\n",
    "    def __init__(self, max_seq_len, token_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_dim = token_dim\n",
    "        \n",
    "    def __call__(self):\n",
    "        even_denom = tf.math.pow(10000,  tf.range(0, self.token_dim, 2)/ token_dim)\n",
    "        odd_denom = tf.math.pow(10000,  tf.range(1, self.token_dim, 2)/ token_dim)\n",
    "        position = tf.expand_dims(tf.range(0, self.max_seq_len, 1, dtype=tf.float64),-1)\n",
    "        pe_even = tf.sin(position / even_denom)\n",
    "        pe_odd = tf.cos(position / odd_denom)\n",
    "        pe = tf.stack([pe_even, pe_odd], axis=2)\n",
    "        pe = tf.reshape(pe, [self.max_seq_len, self.token_dim])\n",
    "        \n",
    "        return pe\n",
    "\n",
    "class MultiheadAttention(tf.Module):\n",
    "    \n",
    "    def __init__(self, nheads, sequence_len, seq_dim):\n",
    "        super().__init__(name=None)\n",
    "        \n",
    "        self.nheads = nheads\n",
    "        self.sequence_len = sequence_len\n",
    "        self.seq_dim = seq_dim\n",
    "        self.head_dim = self.seq_dim // self.nheads\n",
    "        self.qkv_net = tf.keras.layers.Dense(3 * self.seq_dim)\n",
    "        self.fcnn = tf.keras.layers.Dense(self.seq_dim)\n",
    "        self.attention_weights = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, mask=None):\n",
    "        \n",
    "        ## Create a network to generate QKV matrices\n",
    "        assert self.seq_dim == X.shape[-1], \"seaquence dimension given and sequence dimension in the input data is not matching \"\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        QKV = self.qkv_net(X)\n",
    "        print(f\"QKV shape from qkv net: {QKV.shape}\")\n",
    "        QKV = tf.reshape(QKV, QKV.shape[:-1]+[self.nheads, QKV.shape[-1] // self.nheads])\n",
    "        QKV = tf.transpose(QKV, perm=[0,2,1,3])\n",
    "        print(f\"QKV shape after heads added: {QKV.shape}\")\n",
    "        Q, K, V = tf.split(QKV, 3, axis=-1)\n",
    "        print(f\"QKV shape individually: {(Q.shape, K.shape, V.shape)}\")\n",
    "        self.attention_weights, attention_embeddings = scaled_dot_product(Q, K, V, mask)\n",
    "        print(f\"Attention weights and embeddings shape: {(self.attention_weights.shape, attention_embeddings.shape)}\")\n",
    "        batch_size = attention_embeddings.shape[0]\n",
    "        attention_embeddings = tf.reshape(attention_embeddings, shape=[batch_size,self.sequence_len,self.nheads*self.head_dim])\n",
    "        print(f\"Attention embeddings shape before NN: {(self.attention_weights.shape, attention_embeddings.shape)}\")\n",
    "        attention_out  = self.fcnn(attention_embeddings)\n",
    "        print(f\"final attention block output shape: {attention_out.shape}\")\n",
    "        return attention_out\n",
    "        \n",
    "        ## Break that into \n",
    "        \n",
    "        \n",
    "class PositionwiseFeedForward(tf.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob):\n",
    "        super().__init__(name=None)\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        print(f\"x after first linear layer: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"x after activation: {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        print(f\"x after dropout: {x.shape}\")\n",
    "        x = self.linear2(x)\n",
    "        print(f\"x after 2nd linear layer: {x.shape}\")\n",
    "        return x    \n",
    "    \n",
    "class EncoderBlock(tf.Module):\n",
    "    def __init__(self, nheads, sequence_len, seq_dim, hidden_units, drop_prob):\n",
    "        super().__init__(name=None)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.mha = MultiheadAttention( nheads, sequence_len, seq_dim)\n",
    "        self.pff = PositionwiseFeedForward(seq_dim, hidden_units, drop_prob)\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        X_A = layer_norm(X)\n",
    "        X_A = mha(X_A)\n",
    "        X_A = X + X_A\n",
    "        X_F = layer_norm(X_A)\n",
    "        X_F = pff(X_F)\n",
    "        X_out = X_A + X_F\n",
    "        \n",
    "        return X_out\n",
    "        \n",
    "class Encoder(tf.Module):\n",
    "    def __init__(self, nblocks, nheads, sequence_len, seq_dim, hidden_units, drop_prob=0.1 ):\n",
    "        self.encoder_blocks = [EncoderBlock(nheads, sequence_len, seq_dim, hidden_units, drop_prob) for _ in range(nblocks)]\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            X = encoder_block(X)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e735420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "X shape: (16, 10, 512)\n",
      "QKV shape from qkv net: (16, 10, 1536)\n",
      "QKV shape after heads added: (16, 8, 10, 192)\n",
      "QKV shape individually: (TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]), TensorShape([16, 8, 10, 64]))\n",
      "K Shape: (16, 8, 10, 64)\n",
      "Attention weights and embeddings shape: (TensorShape([16, 8, 10, 10]), TensorShape([16, 8, 10, 64]))\n",
      "Attention embeddings shape before NN: (TensorShape([16, 8, 10, 10]), TensorShape([16, 10, 512]))\n",
      "final attention block output shape: (16, 10, 512)\n",
      "x after first linear layer: (16, 10, 1024)\n",
      "x after activation: (16, 10, 1024)\n",
      "x after dropout: (16, 10, 1024)\n",
      "x after 2nd linear layer: (16, 10, 512)\n",
      "encoder output shape: (16, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "token_dim = 512\n",
    "heads = 8\n",
    "\n",
    "X = tf.random.normal(shape=(16,seq_len,token_dim))\n",
    "\n",
    "encoder = Encoder(6, heads, seq_len, token_dim, 1024, 0.1)\n",
    "encoder_out = encoder(X)\n",
    "print(f\"encoder output shape: {encoder_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ef174dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 10, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d72de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
